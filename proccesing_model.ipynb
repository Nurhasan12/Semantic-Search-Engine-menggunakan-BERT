{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa4944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama : Nurhasan\n",
      "NIM  : 32602200014\n",
      "Nama Dataset : Fake and Real News Dataset\n",
      "Link Dataset : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
      "Ada dua dataset fake dan true, keduanya akan digabung, setelah di gabung terdapat beberapa kolom yang akan di analisa seperti :\n",
      "Kolom : text = Judul Column utama yang akan menjadi fokus analisys text\n",
      "Kolom : title = Dapat dibandingkan dengan text untuk analisis ringkas vs panjang\n",
      "Kolom : label(Tambahan) = Untuk tugas klasifikasi → apakah artikel True atau Fake\n"
     ]
    }
   ],
   "source": [
    "print(\"Nama : Nurhasan\")\n",
    "print(\"NIM  : 32602200014\")\n",
    "print(\"Nama Dataset : Fake and Real News Dataset\")\n",
    "print(\"Link Dataset : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\")\n",
    "\n",
    "print(\"Ada dua dataset fake dan true, keduanya akan digabung, setelah di gabung terdapat beberapa kolom yang akan di analisa seperti :\")\n",
    "print(\"Kolom : text = Judul Column utama yang akan menjadi fokus analisys text\")\n",
    "print(\"Kolom : title = Dapat dibandingkan dengan text untuk analisis ringkas vs panjang\")\n",
    "print(\"Kolom : label(Tambahan) = Untuk tugas klasifikasi → apakah artikel True atau Fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ca8d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Na...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Na...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text label  \n",
       "0  Donald Trump just couldn t wish all Americans ...  fake  \n",
       "1  House Intelligence Committee Chairman Devin Nu...  fake  \n",
       "2  On Friday, it was revealed that former Milwauk...  fake  \n",
       "3  On Christmas day, Donald Trump announced that ...  fake  \n",
       "4  Pope Francis used his annual Christmas Day mes...  fake  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data dan menggabungkan dataset\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "true = pd.read_csv(\"True.csv\")\n",
    "\n",
    "# menambahkan column label\n",
    "fake['label'] = 'fake'\n",
    "true['label'] = 'true'\n",
    "\n",
    "# menggabungkan dataset\n",
    "data = pd.concat([fake[['title', 'text', 'label']], true[['title', 'text', 'label']]], ignore_index=True)\n",
    "\n",
    "# Memilih Kolom Yang dibutuhkan\n",
    "data = data[['title', 'text', 'label']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9951d",
   "metadata": {},
   "source": [
    "cleaning\n",
    "Cleaning (pembersihan data) adalah proses untuk mengidentifikasi, memperbaiki, atau menghapus data yang salah, tidak lengkap, tidak relevan, duplikat, atau diformat dengan tidak benar dalam sebuah dataset. Dalam konteks ini, proses cleaning digunakan untuk:\n",
    "\n",
    "Menghapus tanda baca\n",
    "\n",
    "Menghapus angka\n",
    "\n",
    "Menghapus URL\n",
    "\n",
    "Menghapus karakter khusus\n",
    "\n",
    "Mengubah seluruh huruf menjadi huruf kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf40903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>donald trump just couldn t wish all americans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on friday  it was revealed that former milwauk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Na...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on christmas day  donald trump announced that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Na...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...  fake   \n",
       "1  House Intelligence Committee Chairman Devin Nu...  fake   \n",
       "2  On Friday, it was revealed that former Milwauk...  fake   \n",
       "3  On Christmas day, Donald Trump announced that ...  fake   \n",
       "4  Pope Francis used his annual Christmas Day mes...  fake   \n",
       "\n",
       "                                          clean_text  \n",
       "0  donald trump just couldn t wish all americans ...  \n",
       "1  house intelligence committee chairman devin nu...  \n",
       "2  on friday  it was revealed that former milwauk...  \n",
       "3  on christmas day  donald trump announced that ...  \n",
       "4  pope francis used his annual christmas day mes...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)  # menghapus link\n",
    "  text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # menyisakan hanya huruf\n",
    "  text = text.lower()  # mengubah huruf menjadi kecil semua\n",
    "  return text\n",
    "\n",
    "data['clean_text'] = data['text'].astype(str).apply(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537a560",
   "metadata": {},
   "source": [
    "Tokenisasi\n",
    "Tokenisasi adalah proses mengubah teks mentah menjadi unit-unit yang lebih kecil yang disebut token, biasanya berupa kata. Proses ini digunakan untuk membagi urutan teks menjadi bagian-bagian yang lebih mudah dianalisis, seperti membagi kalimat menjadi kata-kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915a6d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\INFINIX\n",
      "[nltk_data]     USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>donald trump just couldn t wish all americans ...</td>\n",
       "      <td>[donald, trump, just, couldn, t, wish, all, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on friday  it was revealed that former milwauk...</td>\n",
       "      <td>[on, friday, it, was, revealed, that, former, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Na...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on christmas day  donald trump announced that ...</td>\n",
       "      <td>[on, christmas, day, donald, trump, announced,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "      <td>[pope, francis, used, his, annual, christmas, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Na...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...  fake   \n",
       "1  House Intelligence Committee Chairman Devin Nu...  fake   \n",
       "2  On Friday, it was revealed that former Milwauk...  fake   \n",
       "3  On Christmas day, Donald Trump announced that ...  fake   \n",
       "4  Pope Francis used his annual Christmas Day mes...  fake   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  donald trump just couldn t wish all americans ...   \n",
       "1  house intelligence committee chairman devin nu...   \n",
       "2  on friday  it was revealed that former milwauk...   \n",
       "3  on christmas day  donald trump announced that ...   \n",
       "4  pope francis used his annual christmas day mes...   \n",
       "\n",
       "                                          token_text  \n",
       "0  [donald, trump, just, couldn, t, wish, all, am...  \n",
       "1  [house, intelligence, committee, chairman, dev...  \n",
       "2  [on, friday, it, was, revealed, that, former, ...  \n",
       "3  [on, christmas, day, donald, trump, announced,...  \n",
       "4  [pope, francis, used, his, annual, christmas, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "data['token_text'] = data['clean_text'].apply(word_tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1acf73",
   "metadata": {},
   "source": [
    "stopword removal\n",
    "Stopword removal adalah proses menghapus kata-kata umum dari teks (seperti \"the\", \"is\", \"in\") yang dianggap tidak memberikan kontribusi signifikan terhadap pemahaman makna atau tujuan analisis teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9259b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\INFINIX\n",
      "[nltk_data]     USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>token_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>donald trump just couldn t wish all americans ...</td>\n",
       "      <td>[donald, trump, just, couldn, t, wish, all, am...</td>\n",
       "      <td>[donald, trump, wish, americans, happy, new, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on friday  it was revealed that former milwauk...</td>\n",
       "      <td>[on, friday, it, was, revealed, that, former, ...</td>\n",
       "      <td>[friday, revealed, former, milwaukee, sheriff,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Na...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on christmas day  donald trump announced that ...</td>\n",
       "      <td>[on, christmas, day, donald, trump, announced,...</td>\n",
       "      <td>[christmas, day, donald, trump, announced, wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "      <td>[pope, francis, used, his, annual, christmas, ...</td>\n",
       "      <td>[pope, francis, used, annual, christmas, day, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Na...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...  fake   \n",
       "1  House Intelligence Committee Chairman Devin Nu...  fake   \n",
       "2  On Friday, it was revealed that former Milwauk...  fake   \n",
       "3  On Christmas day, Donald Trump announced that ...  fake   \n",
       "4  Pope Francis used his annual Christmas Day mes...  fake   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  donald trump just couldn t wish all americans ...   \n",
       "1  house intelligence committee chairman devin nu...   \n",
       "2  on friday  it was revealed that former milwauk...   \n",
       "3  on christmas day  donald trump announced that ...   \n",
       "4  pope francis used his annual christmas day mes...   \n",
       "\n",
       "                                          token_text  \\\n",
       "0  [donald, trump, just, couldn, t, wish, all, am...   \n",
       "1  [house, intelligence, committee, chairman, dev...   \n",
       "2  [on, friday, it, was, revealed, that, former, ...   \n",
       "3  [on, christmas, day, donald, trump, announced,...   \n",
       "4  [pope, francis, used, his, annual, christmas, ...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  [donald, trump, wish, americans, happy, new, y...  \n",
       "1  [house, intelligence, committee, chairman, dev...  \n",
       "2  [friday, revealed, former, milwaukee, sheriff,...  \n",
       "3  [christmas, day, donald, trump, announced, wou...  \n",
       "4  [pope, francis, used, annual, christmas, day, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data['no_stopwords'] = data['token_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b15908",
   "metadata": {},
   "source": [
    "Lemmatization\n",
    "Lemmatization adalah proses mengubah kata berinfleksi (misalnya: running, better) menjadi bentuk dasar atau bentuk kamusnya, yang dikenal sebagai lemma. Ini membantu menyederhanakan kata-kata ke bentuk dasarnya agar analisis teks lebih konsisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f68b8fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\INFINIX\n",
      "[nltk_data]     USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>token_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>donald trump just couldn t wish all americans ...</td>\n",
       "      <td>[donald, trump, just, couldn, t, wish, all, am...</td>\n",
       "      <td>[donald, trump, wish, americans, happy, new, y...</td>\n",
       "      <td>[donald, trump, wish, american, happy, new, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on friday  it was revealed that former milwauk...</td>\n",
       "      <td>[on, friday, it, was, revealed, that, former, ...</td>\n",
       "      <td>[friday, revealed, former, milwaukee, sheriff,...</td>\n",
       "      <td>[friday, revealed, former, milwaukee, sheriff,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Na...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>on christmas day  donald trump announced that ...</td>\n",
       "      <td>[on, christmas, day, donald, trump, announced,...</td>\n",
       "      <td>[christmas, day, donald, trump, announced, wou...</td>\n",
       "      <td>[christmas, day, donald, trump, announced, wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "      <td>[pope, francis, used, his, annual, christmas, ...</td>\n",
       "      <td>[pope, francis, used, annual, christmas, day, ...</td>\n",
       "      <td>[pope, francis, used, annual, christmas, day, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Na...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...  fake   \n",
       "1  House Intelligence Committee Chairman Devin Nu...  fake   \n",
       "2  On Friday, it was revealed that former Milwauk...  fake   \n",
       "3  On Christmas day, Donald Trump announced that ...  fake   \n",
       "4  Pope Francis used his annual Christmas Day mes...  fake   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  donald trump just couldn t wish all americans ...   \n",
       "1  house intelligence committee chairman devin nu...   \n",
       "2  on friday  it was revealed that former milwauk...   \n",
       "3  on christmas day  donald trump announced that ...   \n",
       "4  pope francis used his annual christmas day mes...   \n",
       "\n",
       "                                          token_text  \\\n",
       "0  [donald, trump, just, couldn, t, wish, all, am...   \n",
       "1  [house, intelligence, committee, chairman, dev...   \n",
       "2  [on, friday, it, was, revealed, that, former, ...   \n",
       "3  [on, christmas, day, donald, trump, announced,...   \n",
       "4  [pope, francis, used, his, annual, christmas, ...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [donald, trump, wish, americans, happy, new, y...   \n",
       "1  [house, intelligence, committee, chairman, dev...   \n",
       "2  [friday, revealed, former, milwaukee, sheriff,...   \n",
       "3  [christmas, day, donald, trump, announced, wou...   \n",
       "4  [pope, francis, used, annual, christmas, day, ...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [donald, trump, wish, american, happy, new, ye...  \n",
       "1  [house, intelligence, committee, chairman, dev...  \n",
       "2  [friday, revealed, former, milwaukee, sheriff,...  \n",
       "3  [christmas, day, donald, trump, announced, wou...  \n",
       "4  [pope, francis, used, annual, christmas, day, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['lemmatized_text'] = data['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5b0f0",
   "metadata": {},
   "source": [
    "Representasi text\n",
    "Bag of Words (BoW)\n",
    "Bag of Words adalah metode representasi teks yang menyederhanakan konten dengan menghitung frekuensi kemunculan kata-kata. Dalam representasi ini, struktur atau urutan kata diabaikan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d90afd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbe</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>abide</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yul</th>\n",
       "      <th>zach</th>\n",
       "      <th>zero</th>\n",
       "      <th>zervos</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zinke</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  abandon  abandoned  abbe  abbott  abc  abide  abiding  ability  \\\n",
       "0      0        0          0     0       0    0      0        0        0   \n",
       "1      0        0          0     0       0    0      0        0        0   \n",
       "2      1        0          0     0       0    0      0        0        0   \n",
       "3      0        0          0     0       0    0      0        0        0   \n",
       "4      0        0          0     0       0    0      0        0        0   \n",
       "\n",
       "   able  ...  younger  youth  youtube  yul  zach  zero  zervos  zimbabwe  \\\n",
       "0     0  ...        0      0        0    0     0     0       0         0   \n",
       "1     0  ...        0      0        0    0     0     0       0         0   \n",
       "2     0  ...        0      0        0    0     0     0       0         0   \n",
       "3     0  ...        0      0        0    0     0     0       0         0   \n",
       "4     0  ...        0      0        0    0     0     0       0         0   \n",
       "\n",
       "   zinke  zuckerberg  \n",
       "0      0           0  \n",
       "1      0           0  \n",
       "2      0           0  \n",
       "3      0           0  \n",
       "4      0           0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "bow_matrix = bow_vectorizer.fit_transform(data['lemmatized_text'].apply(lambda x: ' '.join(x)))\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240e8fc",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "TF-IDF adalah metode representasi teks yang tidak hanya menghitung frekuensi kata (seperti BoW), tetapi juga memperhitungkan seberapa penting suatu kata dalam dokumen tertentu relatif terhadap seluruh korpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe0dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Matrix:\n",
      "   aaron  abandon  abandoned  abbe  abbott  abc  abide  abiding  ability  \\\n",
      "0      0        0          0     0       0    0      0        0        0   \n",
      "1      0        0          0     0       0    0      0        0        0   \n",
      "2      1        0          0     0       0    0      0        0        0   \n",
      "3      0        0          0     0       0    0      0        0        0   \n",
      "4      0        0          0     0       0    0      0        0        0   \n",
      "\n",
      "   able  ...  younger  youth  youtube  yul  zach  zero  zervos  zimbabwe  \\\n",
      "0     0  ...        0      0        0    0     0     0       0         0   \n",
      "1     0  ...        0      0        0    0     0     0       0         0   \n",
      "2     0  ...        0      0        0    0     0     0       0         0   \n",
      "3     0  ...        0      0        0    0     0     0       0         0   \n",
      "4     0  ...        0      0        0    0     0     0       0         0   \n",
      "\n",
      "   zinke  zuckerberg  \n",
      "0      0           0  \n",
      "1      0           0  \n",
      "2      0           0  \n",
      "3      0           0  \n",
      "4      0           0  \n",
      "\n",
      "[5 rows x 5000 columns]\n",
      "\n",
      "TF-IDF Matrix:\n",
      "      aaron  abandon  abandoned  abbe  abbott  abc  abide  abiding  ability  \\\n",
      "0  0.000000      0.0        0.0   0.0     0.0  0.0    0.0      0.0      0.0   \n",
      "1  0.000000      0.0        0.0   0.0     0.0  0.0    0.0      0.0      0.0   \n",
      "2  0.051317      0.0        0.0   0.0     0.0  0.0    0.0      0.0      0.0   \n",
      "3  0.000000      0.0        0.0   0.0     0.0  0.0    0.0      0.0      0.0   \n",
      "4  0.000000      0.0        0.0   0.0     0.0  0.0    0.0      0.0      0.0   \n",
      "\n",
      "   able  ...  younger  youth  youtube  yul  zach  zero  zervos  zimbabwe  \\\n",
      "0   0.0  ...      0.0    0.0      0.0  0.0   0.0   0.0     0.0       0.0   \n",
      "1   0.0  ...      0.0    0.0      0.0  0.0   0.0   0.0     0.0       0.0   \n",
      "2   0.0  ...      0.0    0.0      0.0  0.0   0.0   0.0     0.0       0.0   \n",
      "3   0.0  ...      0.0    0.0      0.0  0.0   0.0   0.0     0.0       0.0   \n",
      "4   0.0  ...      0.0    0.0      0.0  0.0   0.0   0.0     0.0       0.0   \n",
      "\n",
      "   zinke  zuckerberg  \n",
      "0    0.0         0.0  \n",
      "1    0.0         0.0  \n",
      "2    0.0         0.0  \n",
      "3    0.0         0.0  \n",
      "4    0.0         0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['lemmatized_text'].apply(lambda x: ' '.join(x)))\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Bag of Words (BoW) Matrix:\")\n",
    "print(bow_df.head())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4041626",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "Word2Vec adalah sebuah teknik dalam pemrosesan bahasa alami (NLP) untuk mempelajari representasi kata berupa vektor numerik.\n",
    "\n",
    "Word2Vec adalah metode pembelajaran representasi kata dalam bentuk vektor berdimensi tetap, yang menangkap makna semantik dan hubungan kontekstual antar kata.\n",
    "\n",
    "Ada dua arsitektur utama:\n",
    "\n",
    "CBOW (Continuous Bag of Words): memprediksi kata berdasarkan konteksnya.\n",
    "\n",
    "Skip-gram: memprediksi konteks berdasarkan kata pusat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df62fe2",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "Melakukan Word2Vec pada satu data pertama Sebagai contoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d72605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektor kata 'donald':\n",
      " [-8.5807729e-01  1.3726141e+00  1.7025287e+00  1.0046636e-01\n",
      "  6.5984684e-01 -7.3875445e-01  9.0431553e-01  2.6612994e-01\n",
      " -2.7818987e-01  1.2702707e+00  1.2578664e+00 -1.3021536e+00\n",
      "  1.7657580e+00  3.6770508e-03  1.3286185e+00 -7.5707573e-01\n",
      " -1.8770985e-01 -1.1120477e+00 -1.0987693e+00 -8.4850264e-01\n",
      "  1.2735498e+00  9.9632096e-01  7.8900719e-01  2.2491392e-02\n",
      "  2.5106412e-01 -2.1282953e-01  3.7721053e-01 -1.2755874e+00\n",
      " -1.4822232e+00 -9.3747658e-01  5.9161717e-01 -1.8253047e-02\n",
      " -3.1237718e-01  1.0644127e+00 -2.2903195e-01  7.7990794e-01\n",
      "  2.9281342e-01 -6.2679338e-01  3.2208145e-01 -3.3253663e+00\n",
      " -1.5520805e-01  3.0057257e-01 -8.1513870e-01 -9.5055115e-01\n",
      " -6.0029620e-01 -3.7861958e-01 -1.5094303e+00  2.5288172e-03\n",
      "  4.0661547e-01  2.4832304e-01  6.7401797e-01 -2.4658875e-01\n",
      "  6.3572049e-01 -1.0936370e+00 -2.4056402e-01 -5.1512963e-01\n",
      " -1.8310428e-01 -5.6036907e-01 -1.3700829e+00  3.7287688e-01\n",
      " -7.9583734e-01  3.8548148e-01 -2.2106187e-02 -1.4834058e+00\n",
      " -1.1628401e+00  1.0687194e+00  5.2036190e-01  1.0479119e-01\n",
      " -1.2849215e+00  2.5928417e-01 -1.1777036e+00  9.6710712e-02\n",
      " -1.0808197e-01  2.8818479e-01 -1.6521013e-01  3.4859180e-01\n",
      "  8.2898849e-01  3.6761591e-01 -2.0923548e+00 -4.8229250e-01\n",
      "  2.6768935e-01 -5.1496285e-01 -9.5320177e-01  2.2645411e+00\n",
      " -1.5593885e+00  3.7821189e-01  1.4824898e+00 -4.1524160e-01\n",
      "  1.2317471e-01  1.2315960e+00  8.9737058e-01 -2.1090569e-01\n",
      "  2.8055452e-02 -1.9603984e-01  1.4870657e+00  9.6045035e-01\n",
      "  9.4280535e-01  5.8070564e-01  8.1765532e-01  1.1004032e+00]\n",
      "\n",
      "Kata-kata yang paling mirip dengan 'donald':\n",
      " [('j', 0.9794216156005859), ('realdonaldtrump', 0.9788307547569275), ('president', 0.960427463054657), ('barack', 0.9367039203643799), ('accuracy', 0.9327276945114136)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# memastikan 'lemmatized_text' berisi list dari token untuk setiap dokumen\n",
    "sentences = data['lemmatized_text'].tolist()\n",
    "\n",
    "# melatih model Word2Vec\n",
    "model_w2v = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Contoh penggunaan: mendapatkan vektor kata dan kata-kata mirip\n",
    "try:\n",
    "    if sentences:\n",
    "        kata_contoh = sentences[0][0] # Ambil kata pertama dari dokumen pertama sebagai contoh\n",
    "        vektor_kata = model_w2v.wv[kata_contoh]\n",
    "        print(f\"Vektor kata '{kata_contoh}':\\n\", vektor_kata)\n",
    "\n",
    "        kata_mirip = model_w2v.wv.most_similar(kata_contoh, topn=5)\n",
    "        print(f\"\\nKata-kata yang paling mirip dengan '{kata_contoh}':\\n\", kata_mirip)\n",
    "    else:\n",
    "        print(\"Tidak ada kalimat dalam data 'lemmatized_text'.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Kata '{e}' tidak ada dalam kosakata model.\")\n",
    "except IndexError:\n",
    "    print(\"Kolom 'lemmatized_text' mungkin kosong atau tidak sesuai format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94219d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     lemmatized_text  \\\n",
      "0  [donald, trump, wish, american, happy, new, ye...   \n",
      "1  [house, intelligence, committee, chairman, dev...   \n",
      "2  [friday, revealed, former, milwaukee, sheriff,...   \n",
      "3  [christmas, day, donald, trump, announced, wou...   \n",
      "4  [pope, francis, used, annual, christmas, day, ...   \n",
      "\n",
      "                                          w2v_vector  \n",
      "0  [-0.40009403, 0.25048673, 0.35660246, -0.09117...  \n",
      "1  [-0.3494197, 0.18689878, 0.4344188, 0.06717452...  \n",
      "2  [-0.34698647, 0.19139728, 0.3498159, -0.011323...  \n",
      "3  [-0.41849443, 0.24946584, 0.3678909, -0.026981...  \n",
      "4  [-0.33008704, 0.13011967, 0.268223, 0.00017564...  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences=data['lemmatized_text'], vector_size=100, window=5, min_count=2)\n",
    "\n",
    "# Representasi rata-rata dari setiap dokumen\n",
    "def get_w2v_vector(tokens):\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "import numpy as np\n",
    "data['w2v_vector'] = data['lemmatized_text'].apply(get_w2v_vector)\n",
    "\n",
    "print(data[['lemmatized_text', 'w2v_vector']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ced93",
   "metadata": {},
   "source": [
    "BERT\n",
    "Bidirectional Encoder Representations from Transformers (BERT) adalah model NLP berbasis transformer dari Google yang membaca teks dua arah (kiri dan kanan), sehingga lebih memahami konteks kata dalam kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d4c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\INFINIX USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memastikan sumber daya NLTK terunduh...\n",
      "Sumber daya NLTK siap.\n",
      "Memuat model dan tokenizer BERT (ini bisa membutuhkan waktu pertama kali)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\INFINIX USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\INFINIX USER\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model BERT siap.\n",
      "\n",
      "Memuat dataset asli (Fake.csv dan True.csv)...\n",
      "Dataset asli dimuat. Contoh teks mentah:\n",
      "0     Donald Trump Sends Out Embarrassing New Yearâ...\n",
      "1     Drunk Bragging Trump Staffer Started Russian ...\n",
      "2     Sheriff David Clarke Becomes An Internet Joke...\n",
      "3     Trump Is So Obsessed He Even Has Obamaâ€™s Na...\n",
      "4     Pope Francis Just Called Out Donald Trump Dur...\n",
      "Name: text_raw, dtype: object\n",
      "Total 1000 dokumen dalam dataset.\n",
      "\n",
      "Melakukan preprocessing pada dataset untuk membuat 'text_processed'...\n",
      "Preprocessing dataset selesai. Contoh teks terproses:\n",
      "0    donald trump sends embarrassing new year eve m...\n",
      "1    drunk bragging trump staffer started russian c...\n",
      "2    sheriff david clarke becomes internet joke thr...\n",
      "3    trump obsessed even obamas name coded website ...\n",
      "4    pope francis called donald trump christmas speech\n",
      "Name: text_processed, dtype: object\n",
      "\n",
      "Memulai proses BERT Embedding untuk semua kalimat. INI AKAN MEMAKAN WAKTU LAMA...\n",
      "BERT Embedding selesai. Bentuk akhir: (1000, 768)\n",
      "Ini berarti ada 1000 kalimat, dan setiap kalimat diwakili oleh 768 angka.\n",
      "\n",
      "Beberapa baris pertama dari DataFrame embeddings yang akan disimpan:\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.375104  0.337601 -0.268276  0.270758 -0.269316 -0.364013  0.657791   \n",
      "1 -0.374765 -0.162393 -0.142094 -0.108183 -0.326707 -0.167158  0.442524   \n",
      "2 -0.336850  0.271862  0.094676  0.027267 -0.067157 -0.246770  0.477563   \n",
      "3 -0.132209  0.061339  0.057988  0.106788 -0.501489 -0.021806  0.389875   \n",
      "4 -0.274587  0.053792  0.260112  0.112551 -0.384794 -0.016082  0.769187   \n",
      "\n",
      "        7         8         9    ...       758       759       760       761  \\\n",
      "0  0.754690 -0.295239 -0.110396  ...  0.139864  0.348960  0.010370  0.314512   \n",
      "1  0.517521 -0.144539 -0.033239  ...  0.037634 -0.155163 -0.007591  0.316897   \n",
      "2  0.263381 -0.152294 -0.085846  ... -0.159759 -0.020595 -0.218781  0.130037   \n",
      "3  0.514109 -0.218759  0.158374  ... -0.240474  0.358939  0.175844  0.202934   \n",
      "4  0.805121 -0.301656 -0.147923  ...  0.269566  0.220992  0.633782  0.320485   \n",
      "\n",
      "        762       763       764       765       766       767  \n",
      "0  0.386093  0.401761  0.310284 -0.397951  0.439389  0.128475  \n",
      "1  0.112953 -0.394949  0.052104 -0.423744  0.294088  0.402595  \n",
      "2 -0.038230  0.315951  0.004183 -0.382202  0.167853  0.173967  \n",
      "3  0.280216 -0.269413  0.201400 -0.737100  0.670404  0.323577  \n",
      "4  0.389015 -0.540753 -0.135958 -0.263620  0.111207 -0.033474  \n",
      "\n",
      "[5 rows x 768 columns]\n",
      "\n",
      "Semua embedding BERT telah disimpan ke 'bert_embeddings_all_sentences.csv'\n",
      "\n",
      "REGENERASI SELESAI!\n",
      "Sekarang Anda bisa kembali menggunakan kode interaktif yang memuat file ini.\n",
      "Pastikan untuk menggunakan 'header=0' saat memuat file ini di kode interaktif Anda.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Unduh semua sumber daya NLTK yang diperlukan di awal ---\n",
    "print(\"Memastikan sumber daya NLTK terunduh...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"Sumber daya NLTK siap.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat mengunduh sumber daya NLTK: {e}\")\n",
    "    print(\"Pastikan Anda memiliki koneksi internet.\")\n",
    "    exit(\"Program dihentikan.\")\n",
    "\n",
    "# --- Inisialisasi NLTK objects ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# --- Inisialisasi Model dan Tokenizer BERT ---\n",
    "print(\"Memuat model dan tokenizer BERT (ini bisa membutuhkan waktu pertama kali)...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval() # Set model ke mode evaluasi untuk inferensi\n",
    "print(\"Model BERT siap.\")\n",
    "\n",
    "\n",
    "# --- Fungsi Preprocessing Lengkap ---\n",
    "# Ini adalah fungsi yang AKAN digunakan untuk membuat kolom 'text_processed'\n",
    "# dan juga untuk memproses query pengguna nanti. Konsistensi sangat penting!\n",
    "def clean_and_process_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan, tokenisasi, menghilangkan stopword, dan lemmatisasi teks.\n",
    "    Output berupa string yang sudah diproses.\n",
    "    \"\"\"\n",
    "    text = str(text) # Pastikan input adalah string\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Hapus non-alfabet dan biarkan spasi\n",
    "    text = text.lower() # Ubah ke huruf kecil\n",
    "\n",
    "    tokens = word_tokenize(text) # Tokenisasi\n",
    "    tokens = [word for word in tokens if word not in stop_words] # Hapus stopword\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens] # Lemmatisasi\n",
    "\n",
    "    return \" \".join(tokens) # Gabungkan kembali menjadi string\n",
    "\n",
    "\n",
    "# --- Fungsi untuk mendapatkan embedding [CLS] dari BERT ---\n",
    "# Fungsi ini akan diterapkan ke setiap teks yang sudah diproses di dataset\n",
    "def get_bert_cls_embedding(text_processed, tokenizer_obj, model_obj):\n",
    "    \"\"\"\n",
    "    Mendapatkan embedding [CLS] dari model BERT untuk satu teks yang sudah diproses.\n",
    "    Mengembalikan array numpy dari embedding.\n",
    "    \"\"\"\n",
    "    if not text_processed.strip():\n",
    "        return np.zeros(model_obj.config.hidden_size)\n",
    "\n",
    "    inputs = tokenizer_obj(text_processed, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_obj(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "# --- AWAL PROSES UTAMA UNTUK REGENERASI EMBEDDING ---\n",
    "\n",
    "# --- Langkah 1: Muat Dataset Asli ---\n",
    "print(\"\\nMemuat dataset asli (Fake.csv dan True.csv)...\")\n",
    "try:\n",
    "    df_fake = pd.read_csv('Fake.csv')\n",
    "    df_true = pd.read_csv('True.csv')\n",
    "\n",
    "    df_fake['label'] = 'fake'  # Tambahkan label untuk berita palsu\n",
    "    df_true['label'] = 'true'  # Tambahkan label untuk berita asli\n",
    "    \n",
    "    df = pd.concat([df_fake, df_true], ignore_index=True)\n",
    "\n",
    "    # Menentukan kolom teks mentah (sesuaikan jika nama kolom Anda berbeda)\n",
    "    if 'title' in df.columns:\n",
    "        df['text_raw'] = df['title']\n",
    "    elif 'text' in df.columns:\n",
    "        df['text_raw'] = df['text']\n",
    "    else:\n",
    "        raise ValueError(\"Tidak ditemukan kolom 'title' atau 'text' di dataset asli. Harap sesuaikan skrip dengan nama kolom teks yang sebenarnya.\")\n",
    "\n",
    "    # Menangani nilai NaN dan string kosong di kolom teks mentah\n",
    "    df['text_raw'] = df['text_raw'].fillna('') # Mengisi NaN dengan string kosong\n",
    "    df['text_raw'] = df['text_raw'].astype(str) # Pastikan semua string\n",
    "    df['text_raw'] = df['text_raw'].replace('nan', '[Teks Kosong Tidak Ditemukan]') # Mengganti string \"nan\"\n",
    "    df['text_raw'] = df['text_raw'].replace('', '[Teks Kosong Tidak Ditemukan]') # Mengganti string kosong\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'Fake.csv' atau 'True.csv' tidak ditemukan.\")\n",
    "    print(\"Membuat DataFrame dummy untuk demonstrasi regenerasi.\")\n",
    "    df = pd.DataFrame({\n",
    "        'text_raw': [\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"A fast brown fox leaps over a sleepy canine.\",\n",
    "            \"The financial market is volatile today.\",\n",
    "            \"Stocks are down, leading to economic uncertainty.\",\n",
    "            \"Interest rates are stable, boosting investor confidence.\",\n",
    "            \"This is a totally unrelated sentence.\",\n",
    "            \"Quick brown dog is running fast.\",\n",
    "            \"Financial news today is very important.\",\n",
    "            \"Donald Trump is a famous political figure.\", # Contoh untuk 'trump'\n",
    "            \"The Pope visited the Vatican.\", # Contoh untuk 'pope'\n",
    "            \"I need some help right now.\" # Contoh untuk 'help me'\n",
    "        ]\n",
    "    })\n",
    "    df['text_raw'] = df['text_raw'].fillna('').astype(str).replace('nan', '[Teks Kosong Tidak Ditemukan]').replace('', '[Teks Kosong Tidak Ditemukan]')\n",
    "\n",
    "\n",
    "print(\"Dataset asli dimuat. Contoh teks mentah:\")\n",
    "print(df['text_raw'].head())\n",
    "print(f\"Total {len(df)} dokumen dalam dataset.\")\n",
    "\n",
    "\n",
    "# --- Langkah 2: Lakukan Preprocessing pada Dataset ---\n",
    "print(\"\\nMelakukan preprocessing pada dataset untuk membuat 'text_processed'...\")\n",
    "df['text_processed'] = df['text_raw'].apply(clean_and_process_text)\n",
    "print(\"Preprocessing dataset selesai. Contoh teks terproses:\")\n",
    "print(df['text_processed'].head())\n",
    "\n",
    "\n",
    "# --- Langkah 3: Hitung BERT Embedding untuk SEMUA Kalimat ---\n",
    "print(\"\\nMemulai proses BERT Embedding untuk semua kalimat. INI AKAN MEMAKAN WAKTU LAMA...\")\n",
    "\n",
    "all_embeddings = df['text_processed'].apply(lambda x: get_bert_cls_embedding(x, tokenizer, model)).tolist()\n",
    "all_embeddings_np = np.vstack(all_embeddings)\n",
    "\n",
    "print(f\"BERT Embedding selesai. Bentuk akhir: {all_embeddings_np.shape}\")\n",
    "print(f\"Ini berarti ada {all_embeddings_np.shape[0]} kalimat, dan setiap kalimat diwakili oleh {all_embeddings_np.shape[1]} angka.\")\n",
    "\n",
    "\n",
    "# --- Langkah 4: Simpan Hasil ke DataFrame dan CSV ---\n",
    "# Buat DataFrame dari semua embeddings\n",
    "bert_embeddings_df_to_save = pd.DataFrame(all_embeddings_np)\n",
    "\n",
    "# Tampilkan beberapa baris pertama dari DataFrame embeddings\n",
    "print(\"\\nBeberapa baris pertama dari DataFrame embeddings yang akan disimpan:\")\n",
    "print(bert_embeddings_df_to_save.head())\n",
    "\n",
    "output_filename = \"bert_embeddings_all_sentences.csv\"\n",
    "# Simpan dengan index=False agar tidak ada kolom index di CSV\n",
    "# Simpan dengan header=True (default) agar ada nama kolom (0, 1, ..., 767)\n",
    "bert_embeddings_df_to_save.to_csv(output_filename, index=False, header=True)\n",
    "print(f\"\\nSemua embedding BERT telah disimpan ke '{output_filename}'\")\n",
    "\n",
    "print(\"\\nREGENERASI SELESAI!\")\n",
    "print(\"Sekarang Anda bisa kembali menggunakan kode interaktif yang memuat file ini.\")\n",
    "print(\"Pastikan untuk menggunakan 'header=0' saat memuat file ini di kode interaktif Anda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19651d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\INFINIX USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memastikan sumber daya NLTK terunduh (ini mungkin membutuhkan waktu sebentar pertama kali)...\n",
      "Sumber daya NLTK siap.\n",
      "Memuat model dan tokenizer BERT (ini bisa membutuhkan waktu pertama kali)...\n",
      "Model BERT siap.\n",
      "Dataset asli dimuat. Contoh:\n",
      "                                            text_raw\n",
      "0   Donald Trump Sends Out Embarrassing New Yearâ...\n",
      "1   Drunk Bragging Trump Staffer Started Russian ...\n",
      "2   Sheriff David Clarke Becomes An Internet Joke...\n",
      "3   Trump Is So Obsessed He Even Has Obamaâ€™s Na...\n",
      "4   Pope Francis Just Called Out Donald Trump Dur...\n",
      "Total 1000 dokumen dalam dataset.\n",
      "\n",
      "Memuat embedding BERT dari 'bert_embeddings_all_sentences.csv'...\n",
      "Embedding dataset dimuat. Bentuk: (1000, 768)\n",
      "\n",
      "--- Sistem Pencarian Teks Mirip dari Dataset ---\n",
      "Ketik teks untuk menemukan dokumen paling mirip dari dataset Anda.\n",
      "Ketik 'keluar' atau 'exit' untuk mengakhiri program.\n",
      "\n",
      "-------------------------------------------\n",
      "Hasil Pencarian Teks Mirip dari Dataset:\n",
      "-------------------------------------------\n",
      "                                                        Teks Mirip (dari Dataset) Label  Skor Kesamaan (Cosine)\n",
      "                      Texas Governor Just Broke With Trump; This Is Unprecedented  fake                0.935938\n",
      " BREAKING: Michael Flynn CRACKS â€“ Will Testify To Mueller Against Trump Himself  fake                0.919855\n",
      "                         Trump says never asked Comey to stop investigating Flynn  true                0.915524\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Hasil Pencarian Teks Mirip dari Dataset:\n",
      "-------------------------------------------\n",
      "                                                 Teks Mirip (dari Dataset) Label  Skor Kesamaan (Cosine)\n",
      "               Texas Governor Just Broke With Trump; This Is Unprecedented  fake                1.000000\n",
      "U.S. judge to lift house arrest for former Trump campaign manager Manafort  true                0.948107\n",
      "                  Republican governors meet with Pence over NAFTA concerns  true                0.948043\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Hasil Pencarian Teks Mirip dari Dataset:\n",
      "-------------------------------------------\n",
      "                                                                         Teks Mirip (dari Dataset) Label  Skor Kesamaan (Cosine)\n",
      "                                          Trump says never asked Comey to stop investigating Flynn  true                1.000000\n",
      "                          Trump travel ban should not apply to people with strong U.S. ties: court  true                0.960513\n",
      " Trumpâ€™s UN Speech May Have Been Terrible, But The Faces Of Other Representatives Were Priceless  fake                0.959684\n",
      "-------------------------------------------\n",
      "Terima kasih! Program diakhiri.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Unduh semua sumber daya NLTK yang diperlukan di awal ---\n",
    "print(\"Memastikan sumber daya NLTK terunduh (ini mungkin membutuhkan waktu sebentar pertama kali)...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"Sumber daya NLTK siap.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat mengunduh sumber daya NLTK: {e}\")\n",
    "    print(\"Pastikan Anda memiliki koneksi internet dan NLTK terinstal dengan benar.\")\n",
    "    exit(\"Program dihentikan.\")\n",
    "\n",
    "# --- Inisialisasi NLTK objects ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# --- Inisialisasi Model dan Tokenizer BERT (hanya untuk query pengguna) ---\n",
    "print(\"Memuat model dan tokenizer BERT (ini bisa membutuhkan waktu pertama kali)...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "print(\"Model BERT siap.\")\n",
    "\n",
    "\n",
    "# --- FUNGSI-FUNGSI PREPROCESSING DAN EMBEDDING (untuk query pengguna) ---\n",
    "\n",
    "# Fungsi Preprocessing Lengkap\n",
    "# Ini harus sama persis dengan yang digunakan saat membuat bert_embeddings_all_sentences.csv\n",
    "def clean_and_process_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan, tokenisasi, menghilangkan stopword, dan lemmatisasi teks.\n",
    "    Output berupa string yang sudah diproses.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Fungsi untuk mendapatkan embedding [CLS] dari BERT (untuk query pengguna)\n",
    "def get_bert_cls_embedding(text_input, tokenizer_obj, model_obj):\n",
    "    if not text_input.strip():\n",
    "        return np.zeros(model_obj.config.hidden_size)\n",
    "\n",
    "    inputs = tokenizer_obj(text_input, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_obj(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "# --- AWAL PROSES UTAMA SKRIP ---\n",
    "\n",
    "# --- Langkah 1: Muat Dataset Asli ---\n",
    "try:\n",
    "    df_fake = pd.read_csv('Fake.csv')\n",
    "    df_true = pd.read_csv('True.csv')\n",
    "\n",
    "    df_fake['label'] = 'fake'  # Tambahkan label untuk berita palsu\n",
    "    df_true['label'] = 'true'  # Tambahkan label untuk berita asli\n",
    "\n",
    "    df = pd.concat([df_fake, df_true], ignore_index=True)\n",
    "\n",
    "    if 'title' in df.columns:\n",
    "        df['text_raw'] = df['title'] # Jangan langsung .astype(str) dulu\n",
    "    elif 'text' in df.columns:\n",
    "        df['text_raw'] = df['text'] # Jangan langsung .astype(str) dulu\n",
    "    else:\n",
    "        raise ValueError(\"Tidak ditemukan kolom 'title' atau 'text' di dataset asli. Harap sesuaikan skrip dengan nama kolom teks yang sebenarnya.\")\n",
    "\n",
    "    # *** PERUBAHAN PENTING DI SINI UNTUK MENGATASI STRING 'nan' ***\n",
    "    # 1. Gunakan fillna() pada nilai NaN yang sebenarnya terlebih dahulu\n",
    "    df['text_raw'] = df['text_raw'].fillna('') # Mengisi NaN dengan string kosong\n",
    "\n",
    "    # 2. Sekarang ubah ke string (ini akan mengubah string 'nan' jika ada)\n",
    "    df['text_raw'] = df['text_raw'].astype(str)\n",
    "\n",
    "    # 3. Ganti string literal 'nan' (jika ada dari data asli) atau string kosong\n",
    "    df['text_raw'] = df['text_raw'].replace('nan', '[Teks Kosong Tidak Ditemukan]')\n",
    "    df['text_raw'] = df['text_raw'].replace('', '[Teks Kosong Tidak Ditemukan]')\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'Fake.csv' atau 'True.csv' tidak ditemukan.\")\n",
    "    print(\"Membuat DataFrame dummy untuk demonstrasi.\")\n",
    "    df = pd.DataFrame({\n",
    "        'text_raw': [\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"A fast brown fox leaps over a sleepy canine.\",\n",
    "            \"The financial market is volatile today.\",\n",
    "            \"Stocks are down, leading to economic uncertainty.\",\n",
    "            \"Interest rates are stable, boosting investor confidence.\",\n",
    "            \"This is a totally unrelated sentence.\",\n",
    "            \"Quick brown dog is running fast.\",\n",
    "            \"Financial news today is very important.\"\n",
    "        ],\n",
    "        'sentiment': ['neutral', 'neutral', 'negative', 'negative', 'positive', 'neutral', 'neutral', 'neutral']\n",
    "    })\n",
    "    # Terapkan penanganan yang sama untuk dummy data\n",
    "    df['text_raw'] = df['text_raw'].fillna('').astype(str).replace('nan', '[Teks Kosong Tidak Ditemukan]').replace('', '[Teks Kosong Tidak Ditemukan]')\n",
    "\n",
    "\n",
    "print(\"Dataset asli dimuat. Contoh:\")\n",
    "print(df[['text_raw']].head())\n",
    "print(f\"Total {len(df)} dokumen dalam dataset.\")\n",
    "\n",
    "\n",
    "# Muat Embedding BERT yang Sudah Ada dari File \n",
    "print(\"\\nMemuat embedding BERT dari 'bert_embeddings_all_sentences.csv'...\")\n",
    "try:\n",
    "    dataset_embeddings_df = pd.read_csv('bert_embeddings_all_sentences.csv', header=0)\n",
    "    dataset_embeddings = dataset_embeddings_df.values\n",
    "\n",
    "    print(f\"Embedding dataset dimuat. Bentuk: {dataset_embeddings.shape}\")\n",
    "\n",
    "    if dataset_embeddings.shape[0] != len(df):\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"PERINGATAN KRITIS: JUMLAH BARIS EMBEDDING TIDAK COCOK DENGAN JUMLAH BARIS DATASET ASLI!\")\n",
    "        print(f\"Dataset asli punya {len(df)} baris, tapi file embedding punya {dataset_embeddings.shape[0]} baris.\")\n",
    "        print(\"Ini akan menyebabkan hasil pencarian salah. Pastikan file embedding Anda sinkron.\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        exit(\"Program dihentikan karena ketidakcocokan data yang serius.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: File 'bert_embeddings_all_sentences.csv' tidak ditemukan.\")\n",
    "    print(\"Anda harus membuat file ini terlebih dahulu dengan menghitung semua embedding dataset.\")\n",
    "    exit(\"Program dihentikan karena file embedding tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat memuat file embedding: {e}\")\n",
    "    exit(\"Program dihentikan.\")\n",
    "\n",
    "\n",
    "# --- Langkah 3: Fungsi Pencarian Interaktif ---\n",
    "def find_similar_texts(user_query, df_data, dataset_emb, top_n=5):\n",
    "    \"\"\"Mencari teks paling mirip di dataset berdasarkan query pengguna.\"\"\"\n",
    "    if not user_query.strip():\n",
    "        print(\"Teks kueri tidak boleh kosong.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    processed_query = clean_and_process_text(user_query)\n",
    "    query_embedding = get_bert_cls_embedding(processed_query, tokenizer, model)\n",
    "\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "    similarities = cosine_similarity(query_embedding, dataset_emb)\n",
    "    similarity_scores = similarities.flatten()\n",
    "    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'Teks Mirip (dari Dataset)': df_data.loc[idx, 'text_raw'],\n",
    "            'Label': df_data.loc[idx, 'label'],  # Tambahkan kolom label\n",
    "            'Skor Kesamaan (Cosine)': similarity_scores[idx]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# --- Loop Interaktif untuk Pengguna ---\n",
    "print(\"\\n--- Sistem Pencarian Teks Mirip dari Dataset ---\")\n",
    "print(\"Ketik teks untuk menemukan dokumen paling mirip dari dataset Anda.\")\n",
    "print(\"Ketik 'keluar' atau 'exit' untuk mengakhiri program.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nMasukkan teks Anda: \")\n",
    "\n",
    "    if user_input.lower() in ['keluar', 'exit']:\n",
    "        print(\"Terima kasih! Program diakhiri.\")\n",
    "        break\n",
    "\n",
    "    if not user_input.strip():\n",
    "        print(\"Input tidak boleh kosong. Silakan masukkan teks.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        similar_texts_df = find_similar_texts(user_input, df, dataset_embeddings, top_n=3)\n",
    "        if not similar_texts_df.empty:\n",
    "            print(\"\\n-------------------------------------------\")\n",
    "            print(\"Hasil Pencarian Teks Mirip dari Dataset:\")\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(similar_texts_df[['Teks Mirip (dari Dataset)', 'Label', 'Skor Kesamaan (Cosine)']].to_string(index=False))\n",
    "            print(\"-------------------------------------------\")\n",
    "        else:\n",
    "            print(\"Tidak ditemukan hasil yang relevan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan saat mencari teks: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
